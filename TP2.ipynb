{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Copie de TP2AntV1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCtfWe5A2vpl",
        "colab_type": "text"
      },
      "source": [
        "# TP2 - Bayes Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnJWQvzo2vpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @title Helper Functions \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def my_gaussian(x_vector,mu,sigma):\n",
        "    \"\"\"\n",
        "  Author: Antoine de Comite \n",
        "  This function computes the gaussian distribution characterised by mu & sigma on the set x_vector\n",
        "  \n",
        "  Inputs : x_vector (numpy array) the set over which you want to compute the gaussian distribution\n",
        "           mu (double) mean value of the gaussian distribution\n",
        "           sigma (double) standard deviation of the gaussian distribution\n",
        "  Outputs: px (numpy array) is the gaussian distribution evaluated over x_vector\n",
        "  \"\"\"\n",
        "  \n",
        "    px = np.exp(- 1/2/sigma**2 * (mu - x_vector) ** 2)\n",
        "    px = px / px.sum()\n",
        "    return px"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_SrJa0J2vp1",
        "colab_type": "text"
      },
      "source": [
        "## Tutorial Objectives\n",
        "\n",
        "At the end of this tutorial you should be able to : \n",
        "\n",
        "1. Understand and implement Bayes' theorem to compute the posterior distribution\n",
        "2. Use Bayes' theorem to infer a human's decision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ1fZXlu2vp5",
        "colab_type": "text"
      },
      "source": [
        "# Section 1 - Bayes' Theorem and the Posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TG3O_IP2vp_",
        "colab_type": "text"
      },
      "source": [
        "Bayes' rule states:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\text{Posterior} = \\dfrac{ \\text{Likelihood} \\times \\text{Prior}}{ \\text{Normalization constant}}\n",
        "\\end{eqnarray}\n",
        "\n",
        "When both the prior and likelihood are Gaussians, this translates into the following form:\n",
        "\n",
        "$$\n",
        "\\begin{array}{rcl}\n",
        "\\text{Likelihood} &=& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\\\\n",
        "\\text{Prior} &=& \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2) \\\\\n",
        "\\text{Posterior} &\\propto& \\mathcal{N}(\\mu_{likelihood},\\sigma_{likelihood}^2) \\times \\mathcal{N}(\\mu_{prior},\\sigma_{prior}^2) \\\\\n",
        "&&= \\mathcal{N}\\left( \\dfrac{\\sigma^2_{likelihood}\\mu_{prior}+\\sigma^2_{prior}\\mu_{likelihood}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}}, \\dfrac{\\sigma^2_{likelihood}\\sigma^2_{prior}}{\\sigma^2_{likelihood}+\\sigma^2_{prior}} \\right) \n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "In these equations, $\\mathcal{N}(\\mu,\\sigma^2)$ denotes a Gaussian distribution with parameters $\\mu$ and $\\sigma^2$:\n",
        "$$\n",
        "\\mathcal{N}(\\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\; \\exp \\bigg( \\frac{-(x-\\mu)^2}{2\\sigma^2} \\bigg)\n",
        "$$\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xK-Y21Q12vqC",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1A: Finding the posterior computationally\n",
        "\n",
        "Imagine an experiment where participants estimate the location of tennis ball. To estimate its position, they can use two sources of information: \n",
        "  1. New noisy visual information (the likelihood)\n",
        "  2. Prior expectations of where the ball is likely to be found (prior). \n",
        "\n",
        "Use Bayes' rule to combine the likelihood (noisy visual information) and the prior to compute the posterior distribution.\n",
        "\n",
        "Hint: \n",
        "\n",
        "* Use the gaussian function my_gaussian() you wrote in the previous tutorial (we already made it available for you in this notebook) to generate a visual likelihood with parameters $\\mu$ = 3 and $\\sigma$ = 1.5\n",
        "* Generate a Gaussian prior with parameters $\\mu$ = -1 and $\\sigma$ = 1.5\n",
        "* Calculate the posterior using pointwise multiplication of the likelihood and prior. Don't forget to normalize so the posterior adds up to 1. **Question : Why do we need to normalize?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEihJYRS2vqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_vector = np.arange(-10,10,0.1)\n",
        "\n",
        "def compute_posterior_pointwise(prior, likelihood):\n",
        "    ''' Author: Florence Blondiaux\n",
        "    Returns the normalized posterior probability based on the prior and the likelihood\n",
        "    Prior: The prior probabilities\n",
        "    Likelihood: The likelihood probabilities\n",
        "    '''\n",
        "    ##########################\n",
        "    ##### Your code here #####\n",
        "    ##########################\n",
        "\n",
        "\n",
        "##########################\n",
        "##### Your code here #####\n",
        "##########################\n",
        "# Plot the three distributions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLyG1j6f_KU1",
        "colab_type": "text"
      },
      "source": [
        "Click [here](https://github.com/fblondiaux/LGBIO2060-2020/blob/master/Solutions/Tp2-Ex1.py) for solution\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2_Ex1.png>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XslhpIQu2vqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "#@markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "x = np.arange(-15, 15, 0.1)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "def refresh(mu_likelihood=3, sigma_likelihood=1.5, mu_prior=-1, sigma_prior=1.5):\n",
        "    likelihood = my_gaussian(x, mu_likelihood, sigma_likelihood)\n",
        "    prior = my_gaussian(x, mu_prior, sigma_prior)\n",
        "    posterior = compute_posterior_pointwise(prior,likelihood)\n",
        "    plt.plot(x, likelihood, 'r')\n",
        "    plt.plot(x,prior,'g')\n",
        "    plt.plot(x, posterior, 'b')\n",
        "\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "_ = widgets.interact(refresh,\n",
        "    mu_posterior=widgets.FloatSlider(value=3, min=-10, max=10, step=0.5, description=\"mu_likelihood:\", style=style),\n",
        "    sigma_posterior=widgets.FloatSlider(value=1.5, min=0.5, max=10, step=0.5, description=\"sigma_likelihood:\", style=style),\n",
        "    mu_prior=widgets.FloatSlider(value=-1, min=-10, max=10, step=0.5, description=\"mu_prior:\", style=style),\n",
        "    sigma_prior=widgets.FloatSlider(value=1.5, min=0.5, max=10, step=0.5, description=\"sigma_prior:\", style=style)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89wCtKoA2vqo",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1B: Finding the posterior analytically\n",
        "\n",
        "**[If you are running short on time, feel free to keep the coding exercise below for later].**\n",
        "\n",
        "The product of two Gaussian distributions, such as our prior and likelihood, remains a Gaussian, regardless of the parameters (**you can verify this property by hand**). We can directly compute the  parameters of that Gaussian from the means and variances of the prior and likelihood. For example, the posterior mean is given by:\n",
        "\n",
        "$$ \\mu_{posterior} = \\frac{\\mu_{likelihood} \\cdot \\frac{1}{\\sigma_{likelihood}^2} + \\mu_{prior} \\cdot \\frac{1}{\\sigma_{prior}^2}}{1/\\sigma_{likelihood}^2 + 1/\\sigma_{prior}^2} \n",
        "$$\n",
        "\n",
        "This expression is a special case for two Gaussians, but is a very useful one because:\n",
        "\n",
        "\n",
        "*   The posterior has the same distribution as both the prior and the likelihood\n",
        "*   There is simple, closed-form expression for its parameters (mean and variance)\n",
        "\n",
        "When these properties hold, we call them **conjugate distributions** or **conjugate priors** (for a particular likelihood). Working with conjugate distributions is very convenient; otherwise, it is often necessary to use computationally-intensive numerical methods to combine the prior and likelihood. \n",
        "\n",
        "In this exercise, we ask you to verify that property.  To do so, we will hold our likelihood constant as an $\\mathcal{N}(3, 1.5)$ distribution, while considering priors with different means ranging from $\\mu=-10$ to $\\mu=10$. For each prior,\n",
        "\n",
        "* Compute the posterior distribution using the function you wrote in Exercise 1A. Next, find its mean. The mean of a probability distribution is $\\int_x p(x) dx$ or $\\sum_x x\\cdot p(x)$. \n",
        "* Compute the analytical posterior mean from likelihood and prior using the equation above.\n",
        "* Plot both estimates of the mean. \n",
        "\n",
        "Are the estimates of the posterior mean the same in both cases? \n",
        "\n",
        "Using these results, try to predict the posterior mean for the combination of a $\\mathcal{N}(-4,4)$ prior and and $\\mathcal{N}(4, 2)$ likelihood. Use the widget above to check your prediction. You can enter values directly by clicking on the numbers to the right of each slider; $\\sqrt{2} \\approx 1.41$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzn6FYAC2vqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_computational_analytical_means():\n",
        "    ##########################\n",
        "    ##### Your code here #####\n",
        "    ##########################\n",
        "    return mu_priors, mus_analytical, mus_by_integration\n",
        "\n",
        "mu_visuals, mu_analytical, mu_computational = compare_computational_analytical_means()\n",
        "##########################\n",
        "##### Your code here #####\n",
        "##########################\n",
        "#Visualize the results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RrN1DJQkkra",
        "colab_type": "text"
      },
      "source": [
        "Click [here](https://github.com/fblondiaux/LGBIO2060-2020/blob/master/Solutions/Tp2-Ex2.py) for solution\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2_Ex2.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOy4d5RW2vq3",
        "colab_type": "text"
      },
      "source": [
        "# Section 2 - Application of Bayes' theorem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZaeMwX2vq4",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "  \n",
        "We'll now estimate the participant response given the posterior distribution. We will describe all the steps of the generative model first. The model will be the same Bayesian model we have been using during the previous exercice: a Gaussian prior and a Gaussian likelihood.\n",
        "\n",
        "Steps:\n",
        "\n",
        "* First, we'll create the prior, likelihood, posterior, etc in a form that will make it easier for us to visualise what is being computed and estimated at each step of the generative model: \n",
        "  1. Generating a Gaussian prior for multiple possible stimulus inputs\n",
        "  2. Generating the likelihood for multiple possible stimulus inputs\n",
        "  3. Estimating our posterior as a function of the stimulus input\n",
        "  4. Estimating a participant response given the posterior\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUVeMzBW2vq4",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![Generative model](https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2-GenerativeModel.png)\n",
        "\n",
        "\n",
        "Here is a graphical representation of the generative model:\n",
        "\n",
        "  1. We present a stimulus $x$ to participants. \n",
        "  2. The brain encodes this true stimulus $x$ noisily (this is the brain's representation of the true visual stimulus: $p(\\tilde x|x)$.\n",
        "  3. The brain then combine this brain encoded stimulus (likelihood: $p(\\tilde x|x)$) with prior information (the prior: $p(x)$) to make up the brain's estimated position of the true visual stimulus, the posterior: $p(x|\\tilde x)$. \n",
        "  3. This brain's estimated stimulus position: $p(x|\\tilde x)$, is then used to make a response:  $\\hat x$, which is the participant's noisy estimate of the stimulus position (the participant's percept). \n",
        "  \n",
        "Typically the response $\\hat x$ also includes some motor noise (noise due to the hand/arm move being not 100% accurate), but we'll ignore it in this tutorial and assume there is no motor noise.\n",
        "\n",
        "We will use the same experimental setup as in the previous exercice. Our subject tries to locates his glasses on his bed stand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgb5TmZ82vq5",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# 1 -  Likelihood array\n",
        "    \n",
        "The first step is to create the likelihood distribution from the encoded stimulus. For the sake of visualization, we will create a likelihood $f(x) = p(\\tilde x|x)$ for each potential encoded stimulus $\\tilde x$. We will then be able to visualize the likelihood as a function of hypothesized true stimulus positions: $x$ on the x-axis and encoded position $\\tilde x$ on the y-axis.\n",
        "\n",
        "**Exercise 2.A**\n",
        "  Using the equation for the `my_gaussian` and the values in `hypothetical_stim`:\n",
        "  \n",
        "* Create a Gaussian likelihood with mean varying from `hypothetical_stim`, keeping $\\sigma_{likelihood}$ constant at 1.\n",
        "* Each likelihood will have a different mean and thus a different row-likelihood of your 2D array, such that you end up with a likelihood array made up of 1,000 row-Gaussians with different means. (_Hint_: `np.tile` won't work here. You may need a for-loop).\n",
        "* Plot the array using the function `plot_myarray()` already pre-written and commented-out in your script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sdk60vTN2vq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.arange(-10, 10, 0.1)\n",
        "hypothetical_stim = np.linspace(-8, 8, 1000)\n",
        "\n",
        "def compute_likelihood_array(x_points, stim_array, sigma=1.):\n",
        "    \"\"\"\n",
        "    This function computes the likelihood array of a given stimulus\n",
        "    \n",
        "    Inputs : x_points (numpy array) is the set of points on which one wants to evaluate the likelihood\n",
        "             stim_array (numpy array) contains the mean position of every possible stimulus\n",
        "             sigma (float) is the standard deviation of the gaussian distribution\n",
        "             \n",
        "    Outputs : likelihood array (numpy array) is a len(stim_array) x len(x_points) that contains the evaluation of \n",
        "              the likelihood for every possible stimulus on the the set x_points. Each line of this array corresponds\n",
        "              to a single simulus (stim_array[i]).\n",
        "    \"\"\"\n",
        "    #########################\n",
        "    ## your code goes here ##\n",
        "    #########################\n",
        "    return likelihood_array \n",
        "\n",
        "\n",
        "# Run the line below to test your code\n",
        "likelihood_array = compute_likelihood_array(x, hypothetical_stim)\n",
        "\n",
        "# Plot the results\n",
        "    #########################\n",
        "    ## your code goes here ##\n",
        "    #########################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZnk0i9PktQZ",
        "colab_type": "text"
      },
      "source": [
        "Click [here](https://github.com/fblondiaux/LGBIO2060-2020/blob/master/Solutions/Tp2-Ex3.py) for solution\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2_Likelihood.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRtb0rIu2vrB",
        "colab_type": "text"
      },
      "source": [
        "**Represents the likelihood distribution for a given stimulus encoding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9Pct7652vrD",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# 2 - Mixture-of-Gaussians Prior\n",
        "\n",
        "The prior distribution can sometimes be more complex than a single Gaussian distribution. In this exercise, we will implement a prior distribution which is a mixture of gaussian distributions. To go back to our tennis example, let us assume that your opponent sends the ball in the middle of the court 90% of the time and in the corner of the court 10% of the time. A mixture of Gaussian will combine these two information to obtain a prior that integrate both information. \n",
        "\n",
        "Here, we will combine those into a mixure-of-Gaussians probability density function (PDF) that captures both possibilties. We will control how the Gaussians are mixed by summing them together with a 'mixing' or weight parameter $p_{centre}$, set to a value between 0 and 1, like so:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\text{Mixture} = \\left(p_{\\text{centre}} \\times \\mathcal{N}\\left(\\mu_{\\text{centre}},\\sigma_{\\text{centre}}\\right)\\right) + \\left(\\left(1-p_{\\text{centre}}\\right) \\times \\mathcal{N}\\left(\\mu_{\\text{corner}},\\sigma_{\\text{corner}}\\right)\\right)\n",
        "\\end{eqnarray}\n",
        "\n",
        "where $p_{centre}$ denotes the probability that the ball lands at the centre of the courrt.\n",
        "\n",
        "\n",
        "\n",
        "For visualization reasons, we will create a prior that has the same shape (form) as the likelihood array we created in the previous exercise. That is, we want to create a mixture of Gaussian prior as a function the the brain encoded stimulus $\\tilde x$. Since the prior does not change as a function of $\\tilde x$ it will be identical for each row of the prior 2D array. \n",
        "\n",
        "Using the equation for the Gaussian `my_gaussian`:\n",
        "* Generate a Gaussian $center$ with mean 0 and standard deviation 0.5. \n",
        "* Generate another Gaussian $corner$ with mean 3 and standard deviation 1\n",
        "* Combine the two Gaussians (center + corner) to make a new prior by mixing the two Gaussians with mixing parameter $p_{corner}$ = 0.50. Make it such that the peakier Gaussian has 50% of the weight (don't forget to normalize afterwards)\n",
        "* This will be the first row of your prior 2D array\n",
        "* Now repeat this for varying brain encodings $\\tilde x$. Since the prior does not depend on $\\tilde x$ you can just repeat the prior for each $\\tilde x$ (hint: use np.tile) that row prior to make an array of 1,000 (i.e. `hypothetical_stim.shape[0]`)  row-priors.\n",
        "* Plot the matrix using the function `plot_myarray()` already pre-written and commented-out in your script\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7BdXql52vrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.arange(-10, 10, 0.1)\n",
        "\n",
        "def calculate_prior_array(x_points, stim_array, p_center,\n",
        "                          prior_mean_center=.0, prior_sigma_center=.5,\n",
        "                          prior_mean_corner=3, prior_sigma_corner=1):\n",
        "    \"\"\"\n",
        "    This function computes the prior distribution based on the mixture of gaussians\n",
        "    \n",
        "    Inputs : x_points (numpy array) is the set of points on which we want to evaluate the prior\n",
        "             stim_array (numpy array) is the array of stimulus input\n",
        "             p_center (float) is the probability that the ball lands in center of the court\n",
        "             mean_center (float) mean value of the center ball landing site\n",
        "             mean_corner (float) mean value of the corner ball landing site \n",
        "             sigma_center (float) std deviation of the center ball landing site\n",
        "             sigma_corner (float) std deviation of the corner ball landing site \n",
        "             \n",
        "    Outputs : prior array (numpy array) contains the prior in a len(stim_array) x len(x_points) array\n",
        "        'indep' stands for independent\n",
        "    \"\"\"\n",
        "\n",
        "    ###########################\n",
        "    ### your code goes here ###\n",
        "    ###########################\n",
        "   \n",
        "    return prior_array\n",
        "\n",
        "\n",
        "# Run the lines below to test your code\n",
        "\n",
        "\n",
        "p_center=.5\n",
        "hypothetical_stim = np.linspace(-8, 8, 1000)\n",
        "prior_array = calculate_prior_array(x, hypothetical_stim, p_center)\n",
        "\n",
        "    ###########################\n",
        "    ### your code goes here ###\n",
        "    ###########################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T3h4Me3aKUIE",
        "colab_type": "text"
      },
      "source": [
        "Click [here](https://github.com/fblondiaux/LGBIO2060-2020/blob/master/Solutions/Tp2-Ex4.py) for solution\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2_Prior.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsnJSNVa2vrO",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Section 3: Bayes Theorem with Complex Posteriors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsW_TNIa2vrR",
        "colab_type": "raw"
      },
      "source": [
        "We now want to compute the posterior distribution using *Bayes Rule*. Since we have already created a likelihood and a prior for each brain encoded position $\\tilde x$, all we need to do is to multiply them row-wise. That is, each row of the posterior array will be the posterior resulting from the multiplication of the prior and likelihood of the same equivalent row.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "    Posterior\\left[i, :\\right] \\propto Likelihood\\left[i, :\\right] \\odot Prior\\left[i, :\\right]\n",
        "\\end{eqnarray}\n",
        "\n",
        "where $\\odot$ represents the [Hadamard Product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)) (i.e., elementwise multiplication) of the corresponding prior and likelihood row vectors `i` from each matrix.\n",
        "\n",
        "Follow these steps to build the posterior as a function of the brain encoded stimulus $\\tilde x$:\n",
        "* For each row of the prior and likelihood (i.e. each possible brain encoding $\\tilde x$), fill in the posterior matrix so that every row of the posterior array represents the posterior density for a different brain encode  $\\tilde x$.\n",
        "\n",
        "**Try to implement a vectorial operation to compute the posterior**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRneAULr2vrS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_posterior_array(prior_array, likelihood_array):\n",
        "    \"\"\"\n",
        "    This function computes the posterior distribution from the prior & the likelihood\n",
        "    \n",
        "    Inputs : prior_array (numpy array) is the prior distribution\n",
        "             likelihood_array (numpy array) is the likelihood distribution\n",
        "             For both these arrays, each line correspond to a different input stimulus\n",
        "    Outputs : posterior_array (numpy array) that contains the posterior distribution for the different\n",
        "              input stimulus (each line corresponds to a different input)\n",
        "    \"\"\"\n",
        "\n",
        "    ###########################\n",
        "    ### your code goes here ###\n",
        "    ###########################\n",
        "\n",
        "    return posterior_array\n",
        "\n",
        "posterior_array = calculate_posterior_array(prior_array, likelihood_array)\n",
        "\n",
        "    ###########################\n",
        "    ### your code goes here ###\n",
        "    ###########################\n",
        "\n",
        "\n",
        "               "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8IHJ-8XKYtB",
        "colab_type": "text"
      },
      "source": [
        "Click [here](https://github.com/fblondiaux/LGBIO2060-2020/blob/master/Solutions/Tp2-Ex5.py) for solution\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2_Posterior.png>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8PirivP2vrY",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# Section 4: Estimating the position $\\hat x$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzw-1nq02vrZ",
        "colab_type": "text"
      },
      "source": [
        "Now that we have a posterior distribution (for each possible brain encoding $\\tilde x$) that represents the brain's estimated ball position: $p(x|\\tilde x)$, we want to make an estimate (response) of the ball location $\\hat x$ using the posterior distribution. This would represent the subject's estimate if their (for us as experimentalist unobservable) brain encoding took on each possible value. \n",
        "\n",
        "This effectively encodes the *decision* that a participant would make for a given brain encoding $\\tilde x$. In this exercise, we make the assumption that participants take the mean of the posterior (decision rule) as a response estimate for the glasses location (use the function `moments_myfunc()` provided to calculate the mean of the posterior).\n",
        "\n",
        "Using this knowledge, we will now represent $\\hat x$ as a function of the encoded stimulus $\\tilde x$. This will result in a 2D binary decision array. To do so, we will scan the posterior matrix (i.e. row-wise), and set the array cell value to 1 at the mean of the row-wise posterior.\n",
        "\n",
        "**Suggestions**\n",
        "* For each brain encoding $\\tilde x$ (row of the posterior array), calculate the mean of the posterior, and set the corresponding cell of the binary decision array to 1. (e.g., if the mean of the posterior is at position 0, then set the cell with x_column == 0 to 1).\n",
        "* Plot the matrix using the function `plot_myarray()` already pre-written and commented-out in your script"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-RE1MNs2vra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_binary_decision_array(x_points, posterior_array):\n",
        "    \"\"\"\n",
        "    This function computes the decision taken by the participants for every potential decision input\n",
        "    \n",
        "    Inputs : x_points (numpy array) is the set of points on which we evaluated the posterior\n",
        "             posterior_array (numpy array) is the posterior distribution\n",
        "             \n",
        "    Outputs : binary_decision_array (numpy array) that contains the decision taken for every potential input stimulus\n",
        "    \"\"\"\n",
        "\n",
        "    binary_decision_array = np.zeros_like(posterior_array)\n",
        "    ###########################\n",
        "    ### your code goes here ###\n",
        "    ###########################\n",
        "\n",
        "\n",
        "    return binary_decision_array\n",
        "\n",
        "binary_decision_array = calculate_binary_decision_array(x, posterior_array)\n",
        "    ###########################\n",
        "    ### your code goes here ###\n",
        "    ###########################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxdP4th3LR6R",
        "colab_type": "text"
      },
      "source": [
        "Click [here](https://github.com/fblondiaux/LGBIO2060-2020/blob/master/Solutions/Tp2-Ex6.py) for solution\n",
        "\n",
        "*Example output:*\n",
        "\n",
        "<img alt='Solution hint' align='left' width=573 height=416 src=https://raw.githubusercontent.com/fblondiaux/LGBIO2060-2020/master/Solutions/TP2_Decision.png>"
      ]
    }
  ]
}